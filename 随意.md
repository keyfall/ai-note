#### 朴素贝叶斯
朴素贝叶斯是分类任务，通过样本属于某一类时概率最大的原理进行分类
基于贝叶斯，是生成式模型，找出特征和类别的联合概率分布然后通过模型，进行分类
主要是求后验概率
![[Pasted image 20240711142814.png]]

朴素的意思是假设所有特征都是相互独立的

计算概率时，可能因为某个特征的数量为0，导致整个概率为0，所以进行平滑处理
![[Pasted image 20240711144554.png]]



#### K最近邻
用于分类和回归，KNN模型就是整个训练集数据，预测时根据特征选择最近的K个点进行决策
重点：
距离计算：使用欧氏距离(两点的直线距离)，曼哈顿距离(路程距离，路程点到点不能使用直线距离)，闵可夫斯基距离
![[Pasted image 20240711151747.png]]
![[Pasted image 20240711151801.png]]
k值选择，一般可以使用k-fold交叉验证方法， 列出一堆K值，把数据集分为K份，K-1份作为训练集，1份作为预测集，重复K次，计算平均准确率，选择性能最好的K值
算出数据集中每个点到预测点的距离，选择最近的N个K值
分类任务通过多数表决定义类别
回归任务通过k个最近距离的平均值作为预测值


如果A数据点包含B数据点，A数据点很多，预测点在B内，因为K值缘故，所以数量上A数据点比B数据点多，那么这时正常会判定预测点为A
这种训练数据不平衡情况可以把距离使用权重值替换，距离近的权重值大，距离远的权重值小，距离转换权重可以使用指数衰减或者距离反比
![[Pasted image 20240711161937.png]]

因为需要计算所有训练数据到预测点距离，很麻烦
可以使用减少计算的方法：仅使用最相关特征进行计算，进行数据归一化，使用numpy加速计算
使用KD树进行处理
构建KD树：计算每个特征的方差值，把方差最大的维度在中位数进行分割，再对于两个矩形进行另一个维度的中位数分割，重复，得到KD树


