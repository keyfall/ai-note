ResNet的核心思想是引入了“残差学习”来解决深度网络中的梯度消失和爆炸问题。
通过添加跳过连接（skip connections）或称为“shortcut connections”，允许网络训练更深层次的架构。

-  结构：ResNet通常由多个相同的残差块组成，每个残差块会有一个跳过连接
	- 多个卷积层
	- 每个卷积层后跟一个批量归一化和ReLU激活函数
	
跳过连接(直接将输入添加到输出上，形成所谓的残差学习。即如果一个残差块的输入为x，经过若干层变换后的输出为F(x)，那么最终的输出就是H(x) = F(x) + x。)

- 优点：
	- 深度：基本超过100层
	- 简化优化：残差学习使得网络可以更好地收敛，即使在网络非常深的情况下也是如此。这是因为学习残差（即差异）比直接学习未经转换的映射要容易得多。
	- 解决梯度消失：跳过连接的y=f(x)+x，反向传播计算梯度，即使f(x)得到的梯度很低，还有一个x进行兜底
	- 宽度和深度灵活性
		- 深度灵活性：
			- 跳跃连接允许网络在深度还有稳定性
			- 残差块结构设计允许只需要叠加残差块就可以增加深度
		- 宽度灵活性：
			- 卷积层的滤波器数量就可以修改输出特征图的通道数，即网络宽度
	- 退化：随着神经网络深度增加，性能更差的一个现象，深层卷积神经网络中尤为明显，通过跳跃连接，使得网络能够轻松学习恒等映射，从而有效地解决了深层网络中的退化问题

- 缺点：
	- 因为深，所以参数多，计算成本高
	- 模型设计复杂性增加
	- 更高的延迟推理时间

#### ResNext
resnet中的卷积改成组卷积
![[Pasted image 20250309222555.png]]
ResNeXt block将之前Resnet block中的3x3普通卷积替换成了group=32的组卷积，并且前两层卷积的out_channel(卷积核个数)都是之前得两倍，如64—>128。
![[Pasted image 20250309223933.png]]

- 优点：
	- 增强的表示能力：通过使用分组卷积，ResNeXt 能够以更高效的方式捕捉输入数据中的更多的特征表示
	- 减少参数数量
	- 模块化设计：ResNeXt 采用了一种称为“cardinality”的超参数来控制分组的数量

- 缺点：
	- 设计复杂度增加
	- 教导成本增加