原理:

&emsp; 训练完后，进行反向传播，从最后一层通过损失函数计算损失值，再把激活函数进行导数，相乘得到结果，结果和权重相乘，最后权重更新
![[Pasted image 20250222161015.png]]

梯度消失原因：
- 激活函数(sigmoid或者tanh)的导数小于1或者趋近0，层数很多导致连乘一个小于1的数，最后梯度趋近0
- 权重初始化时

梯度消失解决：
- 梯度截断：为梯度设置一个阈值，防止梯度过大或过小
- 合理权重初始化
- 使用导数为1或接近1的激活函数:relu,elu,leaky relu
- 批量归一化
- 残差网络
- LSTM和gru
- 自适应学习率


梯度爆炸原因：
- 学习率太高了，导致计算权重和损失值都有些不可控变得不正常
- 权重初始化过大
- 网络层数过多

解决梯度爆炸：
- 权重衰减:通过给参数增加L1或L2范数的正则化项来限制参数的取值范围
- 梯度截断：为梯度设置一个阈值，防止梯度过大或过小
- 合理权重初始化
- 使用导数为1或接近1的激活函数:relu,elu,leaky relu
- 学习率修改
