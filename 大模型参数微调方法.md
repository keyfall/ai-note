#### bitfit:
稀疏的微调方法，它训练时只更新bias的参数或者部分bias参数。
```
import torch 
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_name = "bert-base-uncased" # 示例模型，你可以替换为其他支持的模型
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 创建一个空列表来存储需要更新的参数
params_to_update = []
# 遍历模型的所有参数
for name, param in model.named_parameters():
	if 'bias' in name:
# 如果参数名包含'bias'
		param.requires_grad = True
		params_to_update.append(param)
	else:
		param.requires_grad = False 
optimizer = torch.optim.AdamW(params_to_update, lr=1e-5)
num_epochs = 3  # 训练轮数
for epoch in range(num_epochs):
for batch in dataloader:  # 假设dataloader是你的数据加载器
	inputs, labels = batch
	optimizer.zero_grad()
	
	outputs = model(**inputs)
	loss = outputs.loss  # 假设模型输出包含loss
	
	loss.backward()
	optimizer.step()

```

一般比不上全量参数微调，但是能远超固定参数的方式
#### prefix tuning
在输入向量前增加一个可训练的前缀向量，实现对新任务的适应
```
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# 加载预训练模型和分词器
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 创建前缀向量
prefix_length = 10  # 假设前缀长度为10
prefix = torch.nn.Parameter(torch.randn(prefix_length, model.config.hidden_size))

# 冻结模型参数
for param in model.parameters():
    param.requires_grad = False

# 将前缀向量设置为可训练
optimizer = torch.optim.Adam([prefix], lr=1e-4)

# 假设我们有一个输入文本
input_text = "Once upon a time"
input_ids = tokenizer.encode(input_text, return_tensors='pt')

# 添加前缀
input_with_prefix = torch.cat([prefix.unsqueeze(0).repeat(input_ids.shape[0], 1, 1), input_ids], dim=1)

# 训练过程
for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = model(inputs_embeds=input_with_prefix)
    loss = compute_loss(outputs, targets)  # 这里需要定义一个损失函数
    loss.backward()
    optimizer.step()

# 推理过程
with torch.no_grad():
    generated_ids = model.generate(input_with_prefix, max_length=50)
    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    print(generated_text)
```

优点

1. 计算效率高：
    - 只有前缀向量是可训练的，这大大减少了需要更新的参数数量，从而降低了计算成本。
    - 与全量参数微调相比，Prefix Tuning所需的内存和计算资源更少，适合资源有限的情况。
2. 保持预训练模型的泛化能力：
    - 由于预训练模型的权重保持不变，模型原有的泛化能力和语言理解能力得以保留。
    - 这有助于避免因过度拟合新任务而导致的性能下降。
3. 支持多任务学习：
    - 可以为每个任务设置不同的前缀向量，这样可以轻松地将模型扩展到多个任务上。
    - 不同的任务可以共享同一个预训练模型，只需要为每个任务训练一个较小的前缀向量。
4. 易于实现：
    - Prefix Tuning的实现相对简单，只需在输入序列前添加前缀向量，并仅对这些前缀向量进行优化。
    - 无需修改模型架构或复杂的训练策略。
5. 灵活性：
    - 前缀向量可以设计成不同长度，以适应不同的任务需求。
    - 可以通过调整前缀向量的长度来平衡模型的复杂度和性能。

缺点
1. 性能可能不如全量参数微调：
    - 由于只更新了少量参数，Prefix Tuning可能无法达到全量参数微调那样的性能。
    - 对于某些任务，特别是那些需要大量参数调整的任务，Prefix Tuning可能表现不佳。
2. 依赖于前缀的设计：
    - 前缀向量的设计（如长度、初始化方式等）对最终性能有很大影响。
    - 需要仔细选择前缀的长度和初始化方法，以确保最佳效果。
3. 可能不适用于所有任务：
    - 对于某些特定任务，尤其是那些需要深度语义理解和复杂上下文建模的任务，Prefix Tuning可能不足以提供足够的表达能力。
    - 例如，在长文本生成或复杂的对话系统中，可能需要更多的参数调整。
4. 位置编码和注意力机制的处理：
    - 添加前缀后，需要正确处理位置编码和注意力掩码，以确保模型能够正确地处理前缀部分。
    - 如果处理不当，可能会导致模型性能下降。
    - 后续需要对预测时的输入向量都加上前缀
1. 训练过程中的稳定性问题：
    - 由于前缀向量是随机初始化的，初始阶段可能会出现不稳定的情况。
    - 需要适当的训练策略和超参数调整，以确保训练过程的稳定性和收敛性。

#### prompt tuning
通过设计特定的提示（prompt）来引导模型生成期望的输出
只在输入层加入prompt tokens
```
import torch
from torch import nn
from transformers import BertTokenizer, BertModel, AdamW

# 定义一些参数
model_name = 'bert-base-uncased'
num_labels = 2  # 假设我们有一个二分类任务
prompt_length = 10  # 提示的长度
batch_size = 8
num_epochs = 3
learning_rate = 2e-5

# 加载预训练的BERT模型和分词器
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# 创建一个可学习的软提示
class SoftPrompt(nn.Module):
    def __init__(self, length, embed_dim):
        super(SoftPrompt, self).__init__()
        self.embeddings = nn.Parameter(torch.randn(length, embed_dim))

    def forward(self):
        return self.embeddings

# 初始化软提示
soft_prompt = SoftPrompt(prompt_length, model.config.hidden_size)

# 定义一个简单的分类头
class ClassificationHead(nn.Module):
    def __init__(self, hidden_size, num_labels):
        super(ClassificationHead, self).__init__()
        self.fc = nn.Linear(hidden_size, num_labels)

    def forward(self, x):
        return self.fc(x)

classification_head = ClassificationHead(model.config.hidden_size, num_labels)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = AdamW(list(model.parameters()) + list(soft_prompt.parameters()) + list(classification_head.parameters()), lr=learning_rate)

# 示例数据
texts = ["I love this movie.", "This is a terrible film."]
labels = [1, 0]  # 1表示正面评价，0表示负面评价

# 将文本转换为token
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# 训练循环
for epoch in range(num_epochs):
    optimizer.zero_grad()
    
    # 获取模型的输入嵌入
    input_embeddings = model.embeddings(inputs['input_ids'])
    
    # 在每个样本的输入序列前插入软提示
    prompt_embeddings = soft_prompt().unsqueeze(0).repeat(input_embeddings.size(0), 1, 1)
    combined_embeddings = torch.cat([prompt_embeddings, input_embeddings], dim=1)
    
    # 更新attention mask以包含软提示
    attention_mask = torch.cat([torch.ones((input_embeddings.size(0), prompt_length), dtype=torch.long), inputs['attention_mask']], dim=1)
    
    # 通过模型
    outputs = model(inputs_embeds=combined_embeddings, attention_mask=attention_mask)
    cls_output = outputs.last_hidden_state[:, 0, :]  # 取[CLS] token的输出
    
    # 通过分类头
    logits = classification_head(cls_output)
    
    # 计算损失
    loss = criterion(logits, torch.tensor(labels))
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}")

# 评估或预测部分可以根据需要进一步扩展
```
**优点：**

- **参数效率高**：相较于全模型微调，Prompt Tuning只需优化少量参数，大幅减少了计算资源和内存占用。
- **快速适应性**：由于只需调整少量参数，使得模型能够更快地适应新任务，尤其是在数据量较小的情况下表现良好。
- **易于实现**：不需要对原有模型架构做重大改动，便于集成到现有系统中。
- **可解释性强**：通过精心设计的提示，可以更直观地理解模型如何利用这些信息完成任务。

**缺点：**

- **性能上限**：虽然对于某些任务来说，Prompt Tuning可以达到很好的效果，但在其他复杂度较高的任务上可能无法达到全模型微调所能达到的最佳性能。
- **提示设计难度**：有效的提示设计往往需要一定的专业知识，并且针对不同任务可能需要不同的策略，这增加了使用门槛。
- **泛化能力**：依赖于提示的设计，如果提示不能很好地概括任务特征，则可能导致模型在未见过的数据上的泛化能力下降。
- **任务敏感性**：有些任务可能不适合使用Prompt Tuning方法，特别是那些需要大量上下文信息才能正确理解的任务。
