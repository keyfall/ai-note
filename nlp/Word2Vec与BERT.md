word2vec通过[[词袋模型和skip-gram]]进行训练，得到的一个浅层神经网络嵌入模型,通过预测上下文词汇或者预测目标词汇来学习单词嵌入.因为基于局部窗口，无法捕捉到单词的整体语义和上下文信息，通常用于各种自然语言处理任务的特征表示，一般还需要微调
bert基于transformer架构的预训练语言模型，通过使用大规模的无标注文本数据进行预训练，通过掩盖和预测输入句子中的一部分来学习上下文感知的单词嵌入，基于整个句子的上下文信息来嵌入，使用双向的transform模型来考虑单词的左侧和右侧的上下文信息，因此更全面获取单词的含义，一般不需要微调