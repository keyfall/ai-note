因为随机森林训练后，因为分裂时有很多逻辑，可能会出现过拟合，所以可以牺牲一些训练准确性，让随机森林不那么复杂，方法是剪枝
预剪枝(乐观剪枝)是在训练时进行的，三种规定提前停止分裂:
- 当树的深度超过一个特定值 (最大树深)：防止过拟合，会限制树的深度。最大深度选择方法：在验证集上计算分类误差率，选一个分类误差率最小的深度
- 通过设定某个阈值，如果信息增益大于该阈值，才进行分类，小于该阈值，就停止分裂
- 当内结点包含的数据个数小于一个特定值
实际中预剪枝表现不好，因为过早的停止分裂可能会有欠拟合，就使用后剪枝方法
### 错误率降低剪枝(Reduced-Error Pruning)简称REP方法

REP方法是通过一个新的验证集来纠正树的过拟合问题。对于决策树中的每一个非叶子节点的子树，我们将它替换成一个叶子节点，该叶子节点的类别用大多数原则来确定，这样就产生了一个新的相对简化决策树，然后比较这两个决策树在验证集中的表现。 

### 悲观剪枝法(Pessimistic Error Pruning)简称PEP方法
不使用新的验证集，使用相同的训练样本，通过比较剪枝之后的误判数来决定是否剪枝
剪枝后误判数-剪枝前误判数<剪枝前标准差，那么就认为可以剪枝

![[Pasted image 20250208115615.png]]
![[Pasted image 20250208115647.png]]
这里E(剪枝前误判数)应该是20*0.3

### 代价复杂度算法(Cost-Complexity Pruning)简称为CCP算法
对于每一个非叶子节点进行计算代价复杂度，兄弟子树进行比较，小的那个可以减掉
计算当前非叶子节点的代价复杂度，是通过计算当前节点的错误代价，和子树的错误代价，再通过复杂度进行两个值计算，得到代价复杂度
![[Pasted image 20250208121554.png]]
![[Pasted image 20250208121628.png]]