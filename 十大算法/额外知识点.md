#### 集成学习方法(Boosting、Bagging和Stacking)
集成学习(Ensemble Learning)是机器学习中的一个重要分支，它通过构建并组合多个学习器来完成学习任务，有效地提高了学习系统的泛化能力。
 - boosting:通过迭代的方式，在每一轮训练中，增加对错误样本的关注度
	 - adaboosting:对于每个样本赋予相同权重，训练过程中，被准确分类的样本权重降低，否则权重增加，权重更新后的样本用于下一个分类器训练集，最终训练完毕后，把分类准确率高的分类器权重增加，分类准确率低的分类器权重减少![[Pasted image 20250209222457.png]]ak是第k个弱分类器的权重,ek是加权误差率,I(Gk(xi)≠yi)是指示函数(代表预测和实际值对不上的时候才需要计算)
	 - GBDT：前一个弱分类器得出一个残差(这个残差就是预测值与真实值之间的误差)，然后下一个弱分类器去拟合误差函数对预测值的残差，所有弱分类器的结果相加等于预测值![[Pasted image 20250220165600.png]]![[Pasted image 20250220165638.png]]
	 - XGB：GBTD的改进版，原理一样，增加了一些变动
		 - 正则化:对叶子权重的L2正则化以及对叶子数目的L1正则化，控制模型复杂度，防止过拟合
		 - 自定义损失函数:允许用户根据自己的需求定义可微损失函数
		 - 精确增益计算：使用泰勒二次展开对目标函数近似求解
		 - 并行处理与分布式计算:可以在多线程环境下加速树的构建过程，并且支持分布式计算框架
		 - 处理缺失值: 将缺失值单独作为一类处理，在节点分裂时根据提升度大小决定归类于左叶子节点 或者右叶子节点
		 - 剪枝处理：设置一个参数gamma来决定是否继续分裂节点(预剪枝)
 - bagging：并行训练多个基学习器，每个基学习器在训练集的不同子集上进行训练，最终通过这些基学习器的预测结果进行平均（回归任务）或投票（分类任务）得到最终预测结果
	 - 基本步骤：
		 - 数据采样： 从原始数据中通过自助法随即有放回的抽取多个子集，每个子集的大小与原始数据相同。
		- 训练多个基学习器： 使用不同的子集训练多个基学习器。常见的基学习器是决策树，但也通常可以是其它模型算法
		 - 集成预测： 对于分类问题，通过投票的方式决定最终分类结果，对于回归问题，则通常通过取平均的方式得到最终的预测结果
	- 优点：
		- 解决高方差模型：高方差来源是数据的噪声，模型训练后的预测结果过分依赖训练集，bagging通过有放回采样，减少了数据集噪声影响，通过训练时选取特征子集与预测时投票得到预测结果，增加结果的准确性，减少模型依赖训练集
		- 并行化：因为bagging每个学习器都可以独立训练，所以可以并行化
		- 提升模型稳定性：某些个别模型对特定输入给出了异常值或错误预测，其他模型的正确预测也可以抵消这种影响。
	- 缺点：
		- 计算成本高：Bagging需要训练多个模型，并且通常模型数量越多效果越好，这就导致了更高的计算需求。
		- 高偏差问题：如果基础模型本身具有较高的偏差，那么Bagging可能会加剧这个问题，因为它依赖于平均多个偏向同一方向的估计值。
- stacking: 
	- 基本步骤：
		- 训练基础模型：首先，使用训练数据集训练多个基础模型
		- 模型预测：使用测试数据集对每个基础模型进行预测，得到每个模型的预测结果。
		-特征构造：将每个模型的预测结果作为新的特征，将这些特征进行组合，形成一个元特征。
		- 训练元模型：使用这个新的特征集训练元模型。元模型可以使用任何可用的机器学习算法
		- 预测：使用基础模型先预测，生成的预测结果特征构造为元特征，元模型对元特征进行预测，得到最终的预测结果。
	

	 - 优点：
		- 提高预测性能：Stacking能够综合利用不同基础模型的优点，尤其是当这些模型在不同的数据子集或特征空间上表现良好时。通过次级模型的学习，可以有效地结合这些模型的优势，从而提升整体的预测准确性。
		- 增加模型多样性：使用不同类型的模型作为基础模型可以增加模型的多样性（即模型之间的差异性）。这种多样性有助于减少偏差和方差，增强模型的泛化能力。
		- 处理复杂模式的能力：次级模型可以在基础模型的基础上进一步捕捉数据中的复杂模式。例如，在某些情况下，单个模型可能难以发现的数据模式可以通过次级模型的学习被识别出来。
		- 灵活性高：Stacking允许使用各种不同类型的基础模型和次级模型，可以根据具体问题灵活选择最适合的模型组合。此外，还可以根据需要调整基础模型的数量和类型。
	- 缺点：
		- 计算成本高：Stacking通常涉及训练多个基础模型以及一个次级模型，这显著增加了计算资源的需求。特别是在大型数据集或复杂的模型结构下，计算成本可能会非常高
		- 实现复杂度高：需要考虑如何选择和配置基础模型、如何生成元特征、如何训练次级模型等多个方面的问题
		- 调试难度大：由于涉及到多个模型及其相互作用，定位和解决性能瓶颈或错误变得更加复杂。
- blending：对statcking的简化版，计算成本更低
	- 步骤：
		- 简单直接：Blending是一种较为简单的集成方法，通常只使用训练集的一部分（比如70%）来训练基础模型，然后用剩余部分（未被用于训练基础模型的部分，比如30%）的数据作为验证集来生成元特征。
		- 次级模型训练：次级模型仅基于这个验证集上的基础模型预测结果进行训练，这意味着次级模型从未见过训练集中用于训练基础模型的那一部分数据。

- stacking和blending区别：

|特性|Blending|Stacking|
|:-:|:-:|:-:|
|数据使用方式|划分训练集为训练部分和验证部分|使用K折交叉验证|
|次级模型训练数据|仅基于验证集数据|基于整个训练集的元特征|
|实现难度|较低|较高|
|防止过拟合|可能不足|更好地防止过拟合|
|计算成本|较低|较高|

- boosting，bagging,stacking区别


| 特性               | Boosting                                      | Bagging                                       | Stacking                                      |
|--------------------|-----------------------------------------------|-----------------------------------------------|-----------------------------------------------|
| **基本思想**       | 串行训练，每个模型试图修正前一个模型的错误   | 并行训练，多个模型独立运行，然后取平均或投票 | 使用次级模型来结合多个基础模型的预测结果     |
| **数据处理方式**   | 每个模型在重新加权的数据集上进行训练         | 使用自助采样（Bootstrap Sampling）生成不同数据子集 | 使用K折交叉验证或留出法生成元特征供次级模型训练 |
| **模型依赖性**     | 强依赖，后续模型依赖于前面所有模型的表现     | 无依赖，各模型独立训练                       | 基础模型相互独立，但次级模型依赖于基础模型输出 |
| **目标**           | 减少偏差（bias），适合解决欠拟合问题         | 减少方差（variance），适合解决过拟合问题     | 提高预测性能，通过组合不同模型的优点         |
| **典型算法**       | AdaBoost, GBDT, XGB         | 决策树         | Stacking with Meta-Classifier                |
| **计算复杂度**     | 中等到高，因为需要依次训练多个模型           | 较低到中等，可以并行化训练                   | 高，尤其是当使用交叉验证时                   |
| **防止过拟合**     | 可能需要调节参数以避免过拟合                 | 通过自助采样和多数表决有效减少过拟合         | 通过交叉验证生成元特征有助于防止过拟合       |
| **应用场景**       | 处理具有高偏差的问题                         | 处理具有高方差的问题                         | 综合多种模型的优点，适用于各种场景           |



#### 优化器
- 梯度下降：通过迭代次数的递增，调整使得损失函数最小化的权重。![[Pasted image 20250303065614.png]]
- 随机梯度下降：
	- 原理：使用单个样本或者少量样本更新模型参数，而不使用整个数据集去更新
	- 优点：
#### 损失函数
#### 激活函数
![[Pasted image 20250317103147.png]]
#### full,same,valid卷积操作
[3种卷积操作](https://www.cnblogs.com/itmorn/p/11177439.html)
