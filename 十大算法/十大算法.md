#### 线性回归
一种用于解释自变量与因变量线性关系的预测模型
通过最小化预测值与实际值的平方误差，得到最佳直线，使得误差最小
![[Pasted image 20240701002641.png]]
```
import torch 
from torch import nn 
class LinearRegressionModel(nn.Module): 
def __init__(self, input_dim, output_dim):
	super(LinearRegressionModel, self).__init__()
	self.linear = nn.Linear(input_dim, output_dim)
	
def forward(self, x): 
	out = self.linear(x)
	return out
```

#### 逻辑回归
使用Sigmoid函数将线性回归的输出转换为0到1之间的概率值，进而进行分类决策
这里因为模型方程已经固定了,所以输入x就能得到概率值，只需要进行定义分割线
分割点可以通过数据进行调校，找到发生所有数据时概率最大情况，使用最大似然函数
![[Pasted image 20240701010044.png]]
目的是使得L(θ)最大，得到参数θ的值是多少
为了计算方便，进行负对数似然,把最大化转为最小化，连乘改为连加
```
class LogisticRegression(nn.Module): 
	def __init__(self, input_dim, output_dim): 
		super(LogisticRegression, self).__init__() 
		self.linear = nn.Linear(input_dim, output_dim) 
	
	def forward(self, x): 
		return torch.sigmoid(self.linear(x))

```


#### 线性判别分析(LDA)
主要是降维，也可用于分类
和主成分分析(PCA)的降维区别，LDA是有监督方法(根据类别信息，找到最好区分类别的地方)，PCA是无监督(找到数据方差最大的方向，从而最大化数据的总变异性)
主要分类原理是，通过最大化类别间的距离，和最小化类别内的距离来进行分类
首先计算每一类别的均值向量，然后找到一个方向，使得类别间的散度最大化，同时类别内的散度最小化。


#### 决策树
是一个预测模型，主要用于分类
二叉树结构，每个叶子节点是一个特征选择，每个叶子节点表示类别
使用信息熵、基尼不纯度等准则来选择最佳划分特征
![[Pasted image 20240701223714.png]]


#### 支持向量机
监督学习模型，可用于分类与回归
使用一个超平面把数据以最大间隔分开，目标是最大化类别间的间隔
如果线性不可分的话，可以利用核函数把数据映射到高维空间，线性可分
常用核函数为线性核，多项式核，RBF
软间隔，数据经常不可分，SVM引入松弛变量和惩罚函数C，允许一些数据点违反间隔规则

正负超平面结果1由来：
![[Pasted image 20240714172045.png]]
