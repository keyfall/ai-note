#### 线性回归
一种用于解释自变量与因变量线性关系的预测模型
通过最小化预测值与实际值的平方误差，得到最佳直线，使得误差最小
![[Pasted image 20240701002641.png]]
```
import torch 
from torch import nn 
class LinearRegressionModel(nn.Module): 
def __init__(self, input_dim, output_dim):
	super(LinearRegressionModel, self).__init__()
	self.linear = nn.Linear(input_dim, output_dim)
	
def forward(self, x): 
	out = self.linear(x)
	return out
```

#### 逻辑回归
使用Sigmoid函数将线性回归的输出转换为0到1之间的概率值，进而进行分类决策
这里因为模型方程已经固定了,所以输入x就能得到概率值，只需要进行定义分割线
分割点可以通过数据进行调校，找到发生所有数据时概率最大情况，使用最大似然函数
![[Pasted image 20240701010044.png]]
目的是使得L(θ)最大，得到参数θ的值是多少
为了计算方便，进行负对数似然,把最大化转为最小化，连乘改为连加
```
class LogisticRegression(nn.Module): 
	def __init__(self, input_dim, output_dim): 
		super(LogisticRegression, self).__init__() 
		self.linear = nn.Linear(input_dim, output_dim) 
	
	def forward(self, x): 
		return torch.sigmoid(self.linear(x))

```


#### 线性判别分析(LDA)
主要是降维，也可用于分类
和主成分分析(PCA)的降维区别，LDA是有监督方法(根据类别信息，找到最好区分类别的地方)，PCA是无监督(找到数据方差最大的方向，从而最大化数据的总变异性)
主要分类原理是，通过最大化类别间的距离，和最小化类别内的距离来进行分类
首先计算每一类别的均值向量，然后找到一个方向，使得类别间的散度最大化，同时类别内的散度最小化。


#### 决策树
是一个预测模型，主要用于分类
二叉树结构，每个叶子节点是一个特征选择，每个叶子节点表示类别
使用信息熵、基尼不纯度等准则来选择最佳划分特征
![[Pasted image 20240701223714.png]]

信息增益计算例子：
![[Pasted image 20250125220220.png]]
![[Pasted image 20250125220329.png]]

[基尼系数计算例子](https://blog.csdn.net/2301_77698138/article/details/141438189)
##### id3,c4.5,cart的区别
- 信息增益计算方式
	- ==‌**ID3**‌==：使用信息增益来选择属性，倾向于选择取值较多的属性，容易导致过拟合。‌
	 - ==‌**C4.5**‌==：使用信息增益率来选择属性，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的属性，解决了ID3的过拟合问题。
	- ==‌**CART**‌==：使用基尼系数来选择属性，基尼系数不需要对数运算，更加高效。
- 处理连续特征的能力
	- ==‌**ID3**‌==：只能处理离散特征，无法处理连续特征。‌
	- ==‌**C4.5**‌==：可以处理连续特征，但需要先将特征取值排序，以连续两个值中间值作为划分标准，尝试每一种划分。‌
	- ==‌**CART**‌==：可以处理连续特征，使用基尼系数进行划分。
- 对缺失值的处理方式
	- ==‌**ID3**‌==：对缺失值敏感，处理能力较弱。
	- ==‌**C4.5**‌==：可以通过该样本在其他属性上的分布，计算一个概率权重，将样本划分到不同节点中，但处理过程复杂且耗时较长。‌
	- ==‌**CART**‌==：采用代理分裂来估计缺失值，代理分裂原理尝试使用其他非缺失值的特征作为“代理”来进行分裂，并计算这些分裂与最佳分裂的一致性程度，根据一致性从高到低来进行替代
- 剪枝策略
	- ==‌**C4.5**‌==：采用悲观剪枝方法，通过剪枝来修正树的准确性。([[三个常用后剪枝]])
	- ==‌**CART**‌==：采用“基于代价复杂度剪枝”方法进行剪枝，防止过拟合  

#### 随机森林
原理:
- 从原始数据集样本数量为N，从N个样本中有放回的随机抽取n个样本，形成n个子集，每次未被抽取到的样本组成了n个袋外数据(oob)
- 在每个子集上训练一个决策树。
- 预测时，将n个决策树结合起来，方式为，对需要进行预测的数据进行预测时，根据每个决策树的结果进行投票。如果是分类树，取票数最高的类别，如果是回归树，取各树结果的平均值。 
- 每个决策树的袋外数据，都会对相应的树评估误差，称为袋外误差。

特征重要性:
针对每一个决策树
使用oob作为验证集计算误差，记为erroob1
随机对OOB所有样本的特征X加入噪声干扰（可以随机改变样本在特征X处的值），再次计算袋外数据误差，记为erroob2
假设森林中有N棵树，则特征X的重要性=∑（errOOB2-errOOB1）/N

随机森林选择特征：
1.计算每个特征的重要性，并按降序排序。  
2.确定要剔除的比例(可以设置20-50%)，依据特征重要性剔除相应比例的特征，得到一个新的特征集。  
3.用新的特征集重复上述过程，直到剩下m个特征（m为提前设定的值，可以根据实际情况选择10-20个）。  
4.根据上述过程中得到的各个特征集和特征集对应的袋外误差率，选择袋外误差率最低的特征集。

优点：
- 处理缺失值:
	- 首先，给缺失值预设一些估计值，比如数值型特征，选择其余数据的中位数或众数作为当前的估计值。（CART）
	- 根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径。
	- 判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N。
	- 如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。
- 减少过拟合：多个决策树投票结果(投票结果使用多数准则，数据相同使用随机选择或者加权，加权是因为可能设定时每个决策树权重不一样，具体看各种第三方库内部实现)，每个节点进行分裂时计算的特征用的是部分特征，使得每次分裂不会把决策树过于复杂，让决策树在训练数据上表现很好
- 可以处理高维数据：特征非常多的情况下，也可以有效处理，因为生成训练决策树时，对于每个节点进行分裂时计算的特征用的是部分特征

缺点：
1. 对于噪音大的样本集，容易过拟合
2. 取值比较多的特征容易对RF决策产生更大的影响，从而影响模型效果
3. 对于回归任务表现不好，不能给出连续的输出


#### 支持向量机
监督学习模型，可用于分类与回归
使用一个超平面把数据以最大间隔分开，目标是最大化类别间的间隔
如果线性不可分的话，可以利用核函数把数据映射到高维空间，线性可分
常用核函数为线性核，多项式核，RBF
软间隔，数据经常不可分，SVM引入松弛变量和惩罚函数C，允许一些数据点违反间隔规则

正负超平面结果1由来：
![[Pasted image 20240714172045.png]]
